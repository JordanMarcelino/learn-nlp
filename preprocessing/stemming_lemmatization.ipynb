{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming vs Lemmatization\n",
    "\n",
    "Stemming and lemmatization are similar but it's different. Stemming is applying a set of rules based on the language used to remove affixes. Lemmatization also does the same thing, but it can understand the context. Here's the example for better understanding:\n",
    "\n",
    "Stemming will do this:\n",
    "\n",
    "-   Eating -> Eat\n",
    "-   Solving -> Solv\n",
    "-   Done -> Done\n",
    "\n",
    "Lemmatization will do this:\n",
    "\n",
    "-   Eating -> Eat\n",
    "-   Solving -> Solve\n",
    "-   Done -> Do\n",
    "\n",
    "Well it looks like lemmatization is way better than stemming, then why don't we use lemmatization over stemming? Here's the thing, stemming perform faster than lemmatization because it only apply some set of rules, meanwhile lemmatization is much slower because not only applying some set of rules, it also have a dictionary to look up for base words. Hence both of them have their own advantage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming & Lemmatization with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eating  |  eat\n",
      "eating  |  eat\n",
      "\n",
      "eats  |  eat\n",
      "eats  |  eat\n",
      "eats  |  eat\n",
      "\n",
      "eat  |  eat\n",
      "eat  |  eat\n",
      "eat  |  eat\n",
      "\n",
      "ate  |  ate\n",
      "ate  |  ate\n",
      "ate  |  eat\n",
      "\n",
      "adjustable  |  adjust\n",
      "adjustable  |  adjust\n",
      "adjustable  |  adjustable\n",
      "\n",
      "rafting  |  raft\n",
      "rafting  |  raft\n",
      "rafting  |  raft\n",
      "\n",
      "ability  |  abil\n",
      "ability  |  abil\n",
      "ability  |  ability\n",
      "\n",
      "meeting  |  meet\n",
      "meeting  |  meet\n",
      "meeting  |  meet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \" | \", porter_stemmer.stem(word))\n",
    "    print(word, \" | \", snowball_stemmer.stem(word))\n",
    "    print(word, \" | \", lemmatizer.lemmatize(word, \"v\"))\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization with Spacy\n",
    "\n",
    "Spacy it self doesn't have stemmer as it prefer lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "ate  |  eat\n",
      "adjustable  |  adjustable\n",
      "rafting  |  raft\n",
      "ability  |  ability\n",
      "meeting  |  meeting\n",
      "better  |  well\n"
     ]
    }
   ],
   "source": [
    "text = \"eating eats eat ate adjustable rafting ability meeting better\"\n",
    "docs = nlp(text)\n",
    "\n",
    "for token in docs:\n",
    "    print(token, \" | \", token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise1:\n",
    "\n",
    "-   Convert these list of words into base form using Stemming and Lemmatization and observe the transformations\n",
    "-   Write a short note on the words that have different base words using stemming and Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using stemming in nltk\n",
    "lst_words = [\n",
    "    \"running\",\n",
    "    \"painting\",\n",
    "    \"walking\",\n",
    "    \"dressing\",\n",
    "    \"likely\",\n",
    "    \"children\",\n",
    "    \"whom\",\n",
    "    \"good\",\n",
    "    \"ate\",\n",
    "    \"fishing\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  |  run\n",
      "painting  |  paint\n",
      "walking  |  walk\n",
      "dressing  |  dress\n",
      "likely  |  like\n",
      "children  |  children\n",
      "whom  |  whom\n",
      "good  |  good\n",
      "ate  |  ate\n",
      "fishing  |  fish\n"
     ]
    }
   ],
   "source": [
    "for word in lst_words:\n",
    "    print(word, \" | \", porter_stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running  |  run\n",
      "painting  |  paint\n",
      "walking  |  walk\n",
      "dressing  |  dress\n",
      "likely  |  likely\n",
      "children  |  child\n",
      "who  |  who\n",
      "good  |  good\n",
      "ate  |  eat\n",
      "fishing  |  fishing\n"
     ]
    }
   ],
   "source": [
    "# using lemmatization in spacy\n",
    "doc = nlp(\"running painting walking dressing likely children who good ate fishing\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise2:\n",
    "\n",
    "-   convert the given text into it's base form using both stemming and lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Latha is very multi talented girl.She is good at many skills like dancing, running, singing, playing.She also likes eating Pav Bhagi. she has a \n",
    "habit of fishing and swimming too.Besides all this, she is a wonderful at cooking too.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'latha is veri multi talent girl.sh is good at mani skill like danc , run , sing , playing.sh also like eat pav bhagi . she ha a habit of fish and swim too.besid all thi , she is a wonder at cook too .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using stemming in nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# step1: Word tokenizing\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# step2: getting the base form for each token using stemmer\n",
    "stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# step3: joining all words in a list into string using 'join()'\n",
    "\" \".join(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Latha be very multi talented girl . she be good at many skill like dancing , running , singing , play . she also like eat Pav Bhagi . she have a \\n habit of fishing and swim too . besides all this , she be a wonderful at cook too . \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using lemmatisation in spacy\n",
    "\n",
    "\n",
    "# step1: Creating the object for the given text\n",
    "doc = nlp(text)\n",
    "\n",
    "# step2: getting the base form for each token using spacy 'lemma_'\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "\n",
    "# step3: joining all words in a list into string using 'join()'\n",
    "\" \".join(lemmatized)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
